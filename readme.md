
# ğŸ§  YouTube View Count Simulation using Kafka

This project simulates how **YouTube handles millions of view counts per second** efficiently using **Apache Kafka** as a message broker.  
It demonstrates real-time event streaming, aggregation, and visualization â€” showing how large-scale systems manage heavy traffic without overloading databases.

---

## ğŸš€ Project Overview

When users watch videos, each view is recorded as an **event**.  
Directly writing every event to a database creates bottlenecks and performance issues.  
To solve this, systems like YouTube use **Kafka** to collect, store, and process events asynchronously.

This project recreates that workflow on a smaller scale:

| Component | Description |
|------------|-------------|
| **Client Script** | Simulates user traffic by generating random video view events. |
| **FastAPI Gateway** | Acts as the entry point for requests; pushes events to Kafka. |
| **Kafka Broker** | Buffers and streams all view events in real-time. |
| **Kafka Consumer** | Reads events, aggregates view counts, and writes to SQLite. |
| **SQLite DB** | Stores both aggregated views and detailed event logs. |
| **Streamlit Dashboard** | Displays real-time view counts and traffic analytics. |

---

## ğŸ§© System Architecture

[Client Script / Users] â†“ [FastAPI Gateway] â†“ [Kafka Topic: 'views'] â†“ [Kafka Consumer] â†™          â†˜ [video_stats]  [video_details] (Aggregated)   (Detailed) â†“ [Streamlit Dashboard]

---

## âš™ï¸ Tech Stack

- **Python**
- **FastAPI**
- **Kafka (kafka-python / confluent-kafka)**
- **SQLite**
- **Streamlit**
- **Pandas / Matplotlib**

---

## ğŸ“‚ Project Structure

ğŸ“¦ YTview/ â”‚ â”œâ”€â”€ server.py           # FastAPI app (Kafka producer) â”œâ”€â”€ consumer.py          # Kafka consumer + DB aggregator â”œâ”€â”€ client.py  # Simulates random video views â”œâ”€â”€ monitor.py         # Streamlit dashboard â”œâ”€â”€ requirements.txt     # Python dependencies â””â”€â”€ README.md

---

## ğŸ§  How It Works

1. **Client Simulation**  
   Generates thousands of fake "view" events (video_id, user_id, region, device, timestamp).

2. **FastAPI Gateway (Producer)**  
   Receives HTTP POST requests and pushes events to Kafka topic `views`.

3. **Kafka Broker**  
   Acts as a distributed queue for storing all view events reliably.

4. **Kafka Consumer**  
   Reads events in real-time:  
   - Aggregates total views per video.  
   - Buffers detailed data (user, region, timestamp) for batch insertion.  
   - Periodically flushes both to SQLite.

5. **Streamlit Dashboard**  
   Queries the database and visualizes:  
   - Total views per video (bar/line graph).  
   - Recent detailed view logs.  
   - Real-time updates every few seconds.

---

## ğŸ”§ Installation & Setup

### 1ï¸âƒ£ Prerequisites
- Python 3.9+  
- Kafka & Zookeeper running locally

To start Kafka locally:
```bash
zookeeper-server-start.sh config/zookeeper.properties
kafka-server-start.sh config/server.properties

2ï¸âƒ£ Clone this Repository

git clone https://github.com/yourusername/yt-view-simulation.git
cd yt-view-simulation

3ï¸âƒ£ Install Dependencies

pip install -r requirements.txt

4ï¸âƒ£ Create Kafka Topic

kafka-topics.sh --create --topic views --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1


---

â–¶ï¸ Run the Simulation

Step 1: Start the FastAPI Gateway

uvicorn server:app --reload --port 8000

Step 2: Start the Kafka Consumer

python consumer.py

Step 3: Start the Client Traffic Simulator

python client.py

Step 4: Start the Streamlit Dashboard

streamlit run monitor.py


---

ğŸ“Š Dashboard Features

Real-time total view count per video

Live table of incoming events

Auto-refreshing visualizations

Aggregated vs detailed view comparison



---

ğŸ§  What This Simulates

This project demonstrates how YouTubeâ€™s backend might handle:

Billions of views efficiently using event streaming.

Aggregating data to minimize database writes.

Separating hot (aggregated) and cold (detailed) data for analytics.

Real-time monitoring dashboards for live traffic.



---

ğŸ“ˆ Future Enhancements

Switch SQLite to PostgreSQL for better concurrency.

Add user session tracking & geo-distribution maps.

Implement multi-partition Kafka topics for scaling.

Integrate Spark Streaming or Flink for real-time analytics.



---

ğŸ Conclusion

This simulation shows how large-scale systems like YouTube avoid direct DB writes per view, using Kafka for buffering and aggregation.
Itâ€™s a great starting point for understanding real-time event-driven architectures in production-grade systems.


---

ğŸ§© Author

Kaustubh Gadhave
Backend & AI Engineer | Tech Innovator
âœ¨ Building scalable and intelligent 